---
title: "GLENDR HOW-TO"
author: "Quinn Whiting"
format: pdf
editor: visual
---

# Usage of the glendR R package for GLENDA formatting of PFAS data

## Introduction:

This walk-thru will explain the rationale for the glendR package and how to deal with analyzing the exported SCIEX data in a step-by-step way to create a fully organized dataset for submission to the Great Lakes Environmental Database (GLENDA). A few things to note before we start:

1.  This walk-thru assumes that the user is familiar with R, Rstudio, and the tidyverse packages. If you are not familiar, the R 4 Data Science textbook is a great starting point (<https://r4ds.hadley.nz/>).

2.  This package has dependencies, most of which are within the tidyverse, however few packages should be installed before using glendR

    -   lubridate

    -   ggpmisc

    -   readxl

3.  The order of operations is very importantâ€“ some functions rely on the output of others and changing the order may affect the results or create errors.

### Setting up Rstudio

Using Quarto is advantageous as the R codes are in individual chunks that can be run individually, expensive note taking can be done in the text outside the chunks and allows for others to follow the code easier than commented out lines in RScripts.

1.  Open a new Quart document and name it something appropriate.

2.  Start a new R chunk and load the required packages

```{r}
library(tidyverse)
library(lubridate)
library(ggpmisc)
library(readxl)
library(glendR)
```

The R environment is now set up.

#### Create paths to files:

Copy and paste each file path to the corresponding variable. This allows for changes to be made here instead of throughout the code, keeping things in one place.

Note: some files will still require the "sheet=#" to specify which excel sheet to import.

```{r}
sciexPath<-c("/Users/quinnwhiting/Documents/NRRI/GLSSP/R4GLNDA/Huron/240408_H012C.txt")
infoPath<-c("/Users/quinnwhiting/Documents/NRRI/GLSSP/R4GLNDA/Huron/Reanalysis/H012_test_info.xlsx")
eisnisPath<-c("/Users/quinnwhiting/Documents/NRRI/GLSSP/R4GLNDA/Huron/NIS for EIS.xlsx")
spikePath<-c("/Users/quinnwhiting/Documents/NRRI/GLSSP/R4GLNDA/Huron/PFAS Spikes.xlsx")
mdlPath<-c("/Users/quinnwhiting/Documents/NRRI/GLSSP/R4GLNDA/Huron/MDL ngMl 240316.xlsx")
```

### Importing required datasets

IMPORTANT: after processing the data on SCIEX OS, export the entire results table as a .txt file. Export all rows and all columns. Do not export as a .csv or convert the table to excel, this will change how the data is formatted and the functions associated with glendR will not work.

1.  Import SCIEX data

    -   use the 'loadSciex' function

    -   this creates a large dataframe with all the columns that the processing method from the SCIEX OS has

```{r}
sciex<-loadSciex(file.path = sciexPath)
```

2.  Load excel files of the sample information (example of how the excel file should be formatted)

    -   topOfInterval and bottomOfInterval are the core depths (cm)

    -   remane is used to rename any samples that may have had typos in the SCIEX processing

    -   [there is no function for this in glendR as it is simply loading an excel spreadsheet]{.underline}

| Sample | Weight | topOfInterval | bottomOfInterval | SampleDate | SampleTime | PrepDate | lat | long | depth | rename | batchId |
|------|------|------|------|------|------|------|------|------|------|------|------|
| N001 | 5.07 | NA | NA | 6/28/2023 | 14:27 | 5/13/24 | 43.65436 | -79.65432 | 56.7 | 2023-N001:SS |  |
| N017 | 4.96 | NA | NA | 6/28/2023 | 18:55 | 5/13/24 | 43.876 | -79.2345 | 33.5 | 2023-N017:SS |  |
| OPR1 | 5.01 | NA | NA | NA | NA | 5/13/24 | NA | NA | NA | OPR1 |  |

```{r}
info<-read_excel(path = infoPath)
```

3.  Load an excel sheet that links which EIS and NIS analytes are associated with eachother

    -   again, this is just loading an excel file so there is no function in the glendR package for this

    -   concentration columns are used only if they were not applied in the processing method

        | EIS        | NIS     | NIS.Conc | EIS.Conc |
        |------------|---------|----------|----------|
        | PFBA_M3    | PFBA_M4 | 2.5      | 1        |
        | HFPO-DA_M3 | PFOA_M8 | 2.5      | 1        |
        | PFBS_M2    | PFOS_M8 | 1        | 1        |

```{r}
eis.nis<-read_excel(path = eisnisPath,sheet=3)
#remove any spaces in the names
eis.nis$EIS<-gsub(" ", "", eis.nis$EIS)
eis.nis$NIS<-gsub(" ", "", eis.nis$NIS)
```

4.  Import the method detection limits (MDLs) for each analyte

    -   similar to the previous imports, this is just an excel file

    -   MDLs are in ng/mL, conversion to ng/g can be done with sample weights

```{r}
mdl<-read_excel(mdlPath, sheet = 2)
mdl$Analyte<-gsub(" ","", mdl$Analyte)
```

5.  Import the spike amounts for each EIS/NIS/Native PFAS

    -   EIS and NIS are spiked into every sample and the total mass (ng) needs to be recorded

    -   Target PFAS are also spiked into some QAQC samples and the total mass (ng) needs to be recorded

    -   This file also contains all the CAS# for each PFAS which have to be recorded in the final data file

```{r}
spike<-read_excel(spikePath, sheet=3)
spike$Component<-gsub(" ","", spike$Component)
```

Now, everything should be uploaded to the R Global Environment and it is just a matter of tidying the data and applying flags. How glendR works is to separate the one large dataframe into smaller dataframes by sample type, then performing data manipulations and binding the smaller dataframes back into one large final dataset.

![](images/Slide1.png){fig-align="center"}

## Data Analysis and Initial Formatting:

This section will start the data analysis process (% recoveries, duplicate analyses, QAQC checks, etc) and add the associated flags. These are the small steps that glendR does such that binding back together to the main dataframe will be straight forward and have the correct data.

#### Calibration Curve:

The calibration curve should be checked such that the R2 is \>0.99 and everything looks linear and has enough points. Note: this should have been done on the SCIEX OS, this is just a formality to print off the calibration curve to show that it passed the initial QAQC checks.

IMPORTANT: use 'ggsave' to save the plot to a file location.

```{r}
p1<-calPlot(df=sciex, Title = "##TEXT##", Subtitle = "###text###")
```

#### Detection Limits:

Similar to the calibration curve, the upper and lower detection limits will be found by what is the lowest calibration point for each analyte. Alternatively, the upper limit is found by the highest calibration point used.

LLOD-lower limit (ng/mL)

ULOD- upper limit (ng/mL)

```{r}
dl<-detectionLimits(sciex)
```

### Surrogate Recovery:

To calculate the surrogate (EIS) recovery first a response factor (RF) needs to be calculated between the EIS and NIS, below shows the RF equation from EPA Method 1633:

![](images/RFs%201633%20screenshot.png)

And then to calculate the concentration of the EIS, EPA Method 1633 does:

![](images/EIS%20conc%201633.png)

And then a simple %recovery can be calculated with the known amount spiked into each sample.

This is all done with the 'EISrec' function:

-   the function needs the original dataframe, names of the EIS and NIS, and the eis.nis matching excel file that was imported.

-   it will return a smaller dataframe with the sample name, EIS name, calculated concentration, %recovery, and a matching id used to bind back together later on

```{r}
#get vectors of EIS and NIS
EIS<-unique(eis.nis$EIS)
NIS<-unique(eis.nis$NIS)
#create df of EIS recovery 
df.eis<-EISrec(sciex,EIS,NIS,eis.nis)
```

### NIS Recovery:

Non-extracted Internal Standards (NIS) are spiked in after extraction. For recoveries of NIS, comparison of peak area to the peak area in the calibration curve standards is done. The recovery (peak area) should be \>30% than what is in the standards.

This is all done with the 'NISrec' function:

-   This function only need the original dataframe and the names of the NIS

-   it will return a smaller dataframe with the sample name, NIS name, recovery, and matching id to use to bind back together to the main ddataframe at a later time

```{r}
df.nis<-NISrec(sciex, NIS=NIS)
```

### Ion Ratio:

The ion ratio is the peak area ratio between the qualifier ion and the quantifier ion. Not all PFAS have the quantifier and qualifier ions, and the ratio is only applied when the PFAS is \>2x the LOQ. The ion ratio must be +/- 50% of what the ion ratio is in the standard calibration curve samples.

This is all done in the 'IonRatio' function:

-   only the original dataframe is needed for this function

-   this returns a smaller dataframe with the sample name, component names, the ion ratio, the % difference, and a matching id used later on

```{r}
df.ir<-IonRatio(sciex)
```

### Retention Time:

To be confident in the analyte, the retention time should match the time in the calibration standards.

NOTE: branched isomers may have different retention times and should be considered on a case-by-case basis.

This is all done by the 'retention' function:

-   only the original dataframe is required

-   this returns a smaller dataframe with the sample name, analyte, retention time, the changes from the cal standards (deltaRT), and a matching id used later

```{r}
df.rt<-retention(sciex)
```

## Create a intermediate DF:

Now that the calibration standards and qualifier ions will no longer be used in any calculations, remove them and clean up the original dataframe to an *intermediate* dataframe.

[*Some of the columns are:*]{.underline}

**IS.Actual.Concentration:** the spike amount (ng) added

**Actual.Concentration:** the spike amount added (ng) of the target PFAS

**Calculated.Concentration:** the concentration calculated from the calibration curve (ng/mL)

**Accuracy:** the accuracy (or recovery) of the calculated concentration to the actual spiked in concentration

**Acquisition.Date...Time:** the time of injection

```{r}
#remove unnecessary sample types
df1<-sciex%>%filter(Sample.Type %in% c("Unknown","Quality Control","Blank"))%>%filter(Component.Type !="Qualifiers")
df1<-df1[,c("Sample.Name","Sample.Type","Component.Name","Component.Type","IS.Name","IS.Actual.Concentration","Actual.Concentration","Calculated.Concentration","Acquisition.Date...Time", "Accuracy","Retention.Time")]
df1$Component.Name<-gsub(" ","", df1$Component.Name)
```

### Sample Types:

Creating the sample types is necessary for the functions that will follow, thus this function must be ran first. There are specific QCIDs for sample types. The function 'sampleTypes' in the glendR package creates these.

For the sampleType function:

-   the data frame must have a column named "Sample.Name"

-   it overrides the "Sample.Type" column and renames it "resultQcIdentifier"

-   vectors of unique sample names are required to determine [matrix spikes]{.underline}, [duplicates]{.underline}, f[ield blanks]{.underline}, and [travel blanks]{.underline}

    -   These are *manually* added

    -   if there are none of these sample types in the batch, simply type "NONE" for the sample names (assuming that no actual sample is named 'NONE")

-   Correct naming of samples in the SCIEX OS (during batch set-up) must be used, renaming can be done seperatly if any typos exist. The naming rules are as follows, and case is ignores (i.e., any pattern of upper or lower case will match):

    -   For CLC sample types: "CCV" must be in the name (ex. "CCV1", "CCV_1", "CCV 1". "1CCV", "cCv1")

    -   For LVM sample types: "ISC" must be in the sample name

    -   For CAL sample types: "CAL" must be in the sample name (ex. lowcal 1, Cal_9_check)

    -   For CLB sample types: "IB" must be in the sample name (ex. "IB1", "IB_2")

    -   For LMB sample types: "MB" must be in the sample name

    -   For OPR sample types: "OPR" must be in the sample name (ex. "OPR1", "LLOPR1")

-   it saves the "Sample.Type" column as this will be used in the final dataset and for other functions later on

```{r}
LD1s<-c("H012C_9", "H012C_19")
LD2s<-c("H012C_9_2", "H012C_19_2") #the "_2" naming system is used to signify the duplicate sample
MSs<-c("H012C_20_MS", "H012C_10_MS")
FBs<-c("NONE")
TBs<-c("NONE")

df1<-sampleTypes(df1, MS.Samples = MSs, LD1.Samples = LD1s,LD2.Samples = LD2s, Field.Blanks = FBs, Travel.Blanks = TBs)
```

### Analyte Type:

Change the analyte types to either TRG,SUR, or IS

```{r}
df1<-analyteType(df1, NIS=NIS)
```

### Adding sample information:

Add the additional information for each sample from the info data frame. This includes sample weights, core intervals, sample date and time, preparation dates, location info, and dilutions. Additionally, detection limits (MDLs & LOQs), CAS#s, and spike amounts will be added by the mdl and spike data frames imported previously.

**There is no function for this in glendR** as it is simply adding columns and matching sample names to each other. [The key is to have the exact sample name in the info and sciex files! (look out for pesky spaces that are invisible in excel)]{.underline}

Note: each sample must have a unique name, even the QAQC samples (CCVs, OPRs, blanks, etc.)

[**Sample Weights**]{.underline}

```{r}
df1$weightVolumeAnalyzed<-info$Weight[match(df1$Sample.Name, info$Sample)]
df1$weightVolumeAnalyzed<-if_else(df1$Sample.Type %in% c("CLB","CCV", "CLC","LVM","CAL"), 1, df1$weightVolumeAnalyzed)
df1$weightVolumeUnits<-if_else(df1$Sample.Type %in% c("CLB","CCV", "CLC","LVM","CAL"), "mL", "g")
df1$weightBasis<-if_else(df1$weightVolumeUnits=="g", "DRY", NA)
```

[**Core Intervals**]{.underline}

```{r}
df1$topOfInterval<-info$Upper.Depth[match(df1$Sample.Name, info$Sample)]
df1$bottomOfInterval<-info$Lower.Depth[match(df1$Sample.Name, info$Sample)]
df1$depthUnits<-if_else(is.na(df1$topOfInterval),NA,"cm")
```

[**Dates and Times**]{.underline}

```{r}
df1$sampleDate<-info$Sample.Date[match(df1$Sample.Name, info$Sample)]
df1$sampleTime<-info$Sample.Time[match(df1$Sample.Name, info$Sample)] 
#have to reformat time to remove the date
df1$sampleTime<-as.character(substr(df1$sampleTime, 12,nchar(df1$sampleTime)))
df1$h<-hour(df1$sampleTime)
df1$m<-minute(df1$sampleTime)
df1$time<-paste(df1$h, df1$m, sep = ":")
df1$sampleTime<-df1$time
df1<-df1[,-which(names(df1) %in% c("h","m","time"))]
df1$sampleTime<-gsub("NA:NA",NA, df1$sampleTime)

df1$samplePrepDate<-info$Prep.Date[match(df1$Sample.Name, info$Sample)] 
```

[**Location**]{.underline}

```{r}
df1$lat<-info$Lat[match(df1$Sample.Name, info$Sample)] 
df1$long<-info$Lon[match(df1$Sample.Name, info$Sample)] 
df1$depth_m<-info$depth[match(df1$Sample.Name, info$Sample)] 
```

[**Detection Limits**]{.underline}

match the MDL with the mdl file, the LOQ is calculated by the 'loq' function in the glendR package and only requires a data frame with a mdl column labeled "mdl"

```{r}
#gets the mdl in the matching units (ng/mL or ng/g)
df1$mdl<-round(mdl$`MDL (ng/mL)`[match(df1$Component.Name, mdl$Analyte)]/df1$weightVolumeAnalyzed, digits = 3)

df1<-loq(df1)
```

[**Spike Levels and CAS#**]{.underline}

Don't need to use spike levels from excel sheet if correctly processed the data on SCIEX OS, can use 'Actual.Concentration'

```{r}
df1$spikeLevel<-df1$Actual.Concentration
df1$spikeUnits<-if_else(is.na(df1$spikeLevel), NA, "ng")
df1$analyteCasNum<-spike$CAS[match(df1$Component.Name, spike$Component)]
```

## Results:

Create a results column with the correct concentrations and units. Results should follow this structure:

| Analyte | Analyte Type | Result | Units | Recovery |
|---------|--------------|--------|-------|----------|
| PFBA    | TRG          | 0.32   | ng/g  | NA       |
| PFBA_M3 | SUR          | 0.21   | ng/g  | 102      |
| PFBA_M4 | IS           | NA     | NA    | 63       |

TRG and SUR analytes have results in weight by mass (volume) units while IS analytes do not have any results. Additionally, SUR analytes have a recovery % that is associated with the result and spike amount. IS analytes also have a recovery % associated by peak area (peak areas are not reported in the EDD).

Here, we get to use our previously calculated EIS and NIS recoveries.

```{r}
#create a matching ID 
df1$matchID<-paste(df1$Sample.Name, df1$Component.Name)
#add the EIS concentrations (ng) and %recoveries 
df1$eisConc<-df.eis$Calculated.Concentration[match(df1$matchID, df.eis$matchId)] 
df1$eisRec<-df.eis$sr[match(df1$matchID, df.eis$matchId)]
#add the NIS % recoveries 
df1$nisRec<-df.nis$nisRec[match(df1$matchID, df.nis$matchId)]
```

Now we can work with the TRG analytes

```{r}
df1$trgConc<-if_else(df1$analyteType=="TRG", df1$Calculated.Concentration, NA)
#trg analytes for % recovery
df1$trgLCSrec<-if_else(df1$analyteType=="TRG", if_else(df1$resultQcIdentifier %in% c("CLC","LVM","CAL","OPR"), (df1$Calculated.Concentration/df1$Actual.Concentration*100), NA),NA)
```

Finally, we can combine columns.

```{r}
#get recovery column (note still missing MS recovery, will do later on)
df1$recovery<-paste0(df1$eisRec, df1$nisRec, df1$trgLCSrec)
df1$recovery<-gsub("NA","",df1$recovery)
df1$recovery<-as.numeric(df1$recovery)
df1$recovery<-round(df1$recovery, digits = 1)
#get result column
df1$result<-paste0(df1$eisConc, df1$trgConc)
df1$result<-gsub("NA","",df1$result)
df1$result<-as.numeric(df1$result)
df1$result<-round(df1$result, digits = 5)
#change NA values to 0 for TRG and SUR analytes, update recovery if necessary
df1$result<-if_else(is.na(df1$result), if_else(df1$analyteType=="IS", df1$result, 0), df1$result)
df1$recovery<-if_else(df1$analyteType=="SUR", if_else(df1$result==0.0, 0, df1$recovery), df1$recovery)
df1$recovery<-if_else(df1$resultQcIdentifier %in% c("CLC","LVM","CAL","OPR"), if_else(df1$analyteType=="TRG", if_else(df1$result==0.0, 0, df1$recovery), df1$recovery),df1$recovery)
```

## Duplicate analysis:

At least 1 of 10 samples are extracted and analyzed in duplicate. The relative percent difference (RPD) is calculated between each analyte. This uses the ng/g concentration as the sample weights may differ and cause differences in the final ng/mL concentration measured by the instrument. the RPD must be \<40% for analytes that have higher detection (\>3xMDL).

**The function 'rpd' in the glendR package** calculates the RPD and associates them to each sample-analyte combo.

```{r}

```
